{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fa350f-3b3b-4fc8-a090-0245880cb34e",
   "metadata": {},
   "source": [
    "# 利用llama_index+ipex_llm快速开发基于RAG技术的笔记本产品说明书和电脑配置助手"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917eb069-b0e7-40c3-937c-3c74c727e654",
   "metadata": {},
   "source": [
    "### 安装依赖 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a0fa2-0c12-4778-8fd1-5fd9b98bee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719074a-7ce2-489c-81c1-7c3a1d3b993c",
   "metadata": {},
   "source": [
    "### 加载分词模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f342167-2b6f-439d-8796-6ee1069010f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载本地模型\n",
    "tokenizer_model_id_or_path = r\"D:\\models\\Qwen\\Qwen2___5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_id_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ced1cb-eac3-4ffa-8570-e223ec8073b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from D:\\models\\Qwen\\Qwen2___5-7B-Instruct-GGUF\\qwen2.5-7b-instruct-q2_k.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = qwen2.5-7b-instruct\n",
      "llama_model_loader: - kv   3:                            general.version str              = v0.1\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = qwen2.5-7b-instruct\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q2_K:  113 tensors\n",
      "llama_model_loader: - type q3_K:   56 tensors\n",
      "llama_model_loader: - type q4_K:   28 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3584\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 28\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18944\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 7.62 B\n",
      "llm_load_print_meta: model size       = 2.80 GiB (3.16 BPW) \n",
      "llm_load_print_meta: general.name     = qwen2.5-7b-instruct\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q2_K) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  2870.55 MiB\n",
      "..................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 3904\n",
      "llama_new_context_with_model: n_ctx_per_seq = 3904\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (3904) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:        CPU KV buffer size =   213.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  213.50 MiB, K (f16):  106.75 MiB, V (f16):  106.75 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   304.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 986\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'qwen2.5-7b-instruct', 'general.architecture': 'qwen2', 'general.type': 'model', 'general.finetune': 'qwen2.5-7b-instruct', 'general.version': 'v0.1', 'qwen2.block_count': '28', 'general.size_label': '7.6B', 'qwen2.context_length': '131072', 'qwen2.embedding_length': '3584', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.feed_forward_length': '18944', 'qwen2.attention.head_count': '28', 'qwen2.attention.head_count_kv': '4', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'general.file_type': '10', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "model_path = r\"D:\\models\\Qwen\\Qwen2___5-7B-Instruct-GGUF\\qwen2.5-7b-instruct-q2_k.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=None,\n",
    "    model_path=model_path,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    "    verbose=True,\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e5e7d-62e2-439e-b2a3-9f76b589a951",
   "metadata": {},
   "source": [
    "## 接下来，使用增强检索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c4e93-0cd6-4d22-b15f-21690d5ecc3e",
   "metadata": {},
   "source": [
    "### 使用set_global_tokenizer配置分词模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc0a0a4-70fd-4953-ac01-4270c9503e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_model_id_or_path = r\"D:\\models\\Qwen\\Qwen2___5-7B-Instruct\"\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(tokenizer_model_id_or_path).encode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734a6a4-b008-45f0-9057-1b1bcc83e3ec",
   "metadata": {},
   "source": [
    "### 使用HuggingFaceEmbedding配置embed模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2dd90ae-8fe9-4d0b-87b4-17387f38ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model_path = r\"D:\\models\\BAAI\\bge-m3\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_path)\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2bbea-1eb8-4a83-8980-95e00b89858e",
   "metadata": {},
   "source": [
    "### 加载PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8282943-90f7-4749-b945-c900f74ac86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e6ace2-2c28-4666-b73e-0e3a4338e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档 ID: f498ee9c-f075-4608-b46e-a2ed445a5500\n",
      "文档内容: P/N: 147003835\n",
      "© 版权所有 联想 2010\n",
      "Lenovo\n",
      "安全及通用信息指南\n",
      "www.lenovo.com.cn\n",
      "New World. New Thinking.\n",
      "TM\n",
      "--------------------------------------------------\n",
      "文档 ID: 4e126f84-5fb9-4eab-bbc4-9ab24bab1a17\n",
      "文档内容: \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 查看前两端文档内容\n",
    "for doc in documents[:2]:\n",
    "    print(f\"文档 ID: {doc.doc_id}\")\n",
    "    print(f\"文档内容: {doc.text}\")\n",
    "    print(\"-\" * 50)  # 分隔不同的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac6db37-007d-47e8-ab83-e5801730b3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档 ID: 8982bf3f-abf0-41a4-9b57-dfbd6cb8cc53\n",
      "文档内容: 23. 联想 IdeaPad 3 Gen 8：AMD Ryzen 5 7535U，8GB DDR4，512GB SSD，15.6 英\n",
      "寸 FHD 屏，续航可达 10 小时 3。 \n",
      "24. 联想 IdeaPad Flex 5i：Intel Core i5 1335U，16GB LPDDR4X，1TB SSD，14 英\n",
      "寸 FHD 触控屏，续航可达 12 小时 3。 \n",
      "25. 联想 Yoga 9i Gen 8：Intel Core i9 13900H，32GB LPDDR5，2TB PCIe NVMe SSD，\n",
      "14 英寸 4K OLED 触控屏，续航可达 14 小时 3。 \n",
      "26. 联想 Legion 5 Pro：AMD Ryzen 7 5800H，16GB DDR4，1TB SSD，NVIDIA GeForce \n",
      "RTX 3060 显卡，16 英寸 QHD、165Hz 屏，续航可达 8 小时 3。 \n",
      "27. 联想 ThinkPad X390 ：英特尔酷睿  i5 或 i7 处理器， 8GB 或 16GB 内存，\n",
      "512GB 或 1TB SSD， 集成显卡，13.3 英寸高分辨率屏幕， 适合经常出差人士2。 \n",
      "28. 联想 ThinkPad X395：AMD 锐龙处理器，8GB 或 16GB 内存，512GB 或 1TB \n",
      "SSD，集成显卡，13.3 英寸高分辨率屏幕，轻薄便携，续航出色 2。 \n",
      "29. 联想 Yoga C940：英特尔酷睿 i7 或 i9 处理器，16GB 或 32GB 内存，512GB \n",
      "或 1TB SSD，4K 触控屏，杜比全景声音响系统，支持 360 度翻转 2。 \n",
      "30. 联想 Yoga S740： 英特尔酷睿 i7 处理器，16GB 内存，512GB 或 1TB SSD，高\n",
      "分辨率屏幕，长效电池，适合日常办公娱乐 2。 \n",
      "31. 联想 Yoga Duet 7i： 英特尔处理器，8GB 或 16GB 内存，256GB 或 512GB SSD，\n",
      "可拆卸键盘，支持手写笔输入，高分辨率触控屏 2。 \n",
      "32. 联想 Legion Y740： 英特尔酷睿 i7 处理器，16GB 内存，512GB 或 1TB SSD，\n",
      "NVIDIA GeForce RTX 2070 显卡，144Hz 高刷新率屏幕 2。 \n",
      "33. 联想 Legion Y540： 英特尔酷睿 i5 处理器，8GB 或 16GB 内存，512GB SSD，\n",
      "GTX 1650 显卡，适合预算有限的游戏玩家 2。 \n",
      "34. 联想 Legion Y9000X： 英特尔酷睿 i9 处理器，16GB 或 32GB 内存，1TB SSD，\n",
      "RTX 2080 显卡，高分辨率显示屏 2。 \n",
      "35. 联想 ThinkBook Plus： 英特尔酷睿 i7 处理器，16GB 内存，512GB 或 1TB SSD，\n",
      "配备电子墨水屏，支持手写笔输入 2。 \n",
      "36. 联想 IdeaPad 5：AMD Ryzen 5 处理器，8GB 或 16GB 内存，512GB SSD，高\n",
      "清屏幕，长效电池 2。 \n",
      "37. 联想 ThinkPad T490 ：英特尔酷睿  i5 或 i7 处理器， 8GB 或 16GB 内存，\n",
      "512GB 或 1TB SSD，高清屏幕，丰富接口，适合长时间办公 2。 \n",
      "38. 联想 ThinkPad T590 ：英特尔酷睿  i5 或 i7 处理器， 8GB 或 16GB 内存，\n",
      "512GB 或 1TB SSD，15.6 英寸高清屏幕，性能与便携性兼顾 2。 \n",
      "39. 联想小新 Air14 2019： 英特尔酷睿 i5 处理器，8GB 或 16GB 内存，512GB SSD，\n",
      "14 英寸屏幕，窄边框设计 1。 \n",
      "40. 联想 YOGA Air 15 Aura AI 元启版：具体 CPU 型号未知，内存规格较高，大\n",
      "容量 SSD，屏幕分辨率高，可能有独立显卡，集成 AI 功能 1。 \n",
      "41. 联想拯救者 Y9000P 2024： 酷睿 i9 - 14900HX，16GB 内存，1TB SSD，RTX4060 \n",
      "显卡，屏幕高分辨率、高刷新率 4。 \n",
      "42. 联想拯救者 Y7000 2024： 酷睿 i7 - 13650HX，24GB 内存，512GB SSD，RTX4060 \n",
      "显卡，15.6 英寸屏幕 4。 \n",
      "43. 联想小新 16 2024 锐龙版：R7 - 8845H，16GB 内存，512GB SSD，16 英寸屏\n",
      "幕 4。 \n",
      "44. 联想 ThinkBook 16+ 2024 锐龙版：R7 - 8845H，16GB 内存，1TB SSD4。 \n",
      "45. 联想小新 Pro16 2024 锐龙版：R7 - 8845H，16GB 内存，1TB SSD，2.5K 屏幕\n",
      "--------------------------------------------------\n",
      "文档 ID: b377cd64-4132-40bd-9bbd-019f93739f4d\n",
      "文档内容: 4。 \n",
      "46. 联想 ThinkPad X1 (1293AQ4)：英特尔酷睿 i5 2 代系列（Sandy Bridge），4GB \n",
      "内存，500GB 硬盘，屏幕 13.3 英寸，核芯显卡 Intel GMA HD 3000。 \n",
      "47. 联想 Legion 7i 2023 款： 英特尔酷睿 i9 处理器，32GB 内存，1TB SSD，NVIDIA \n",
      "GeForce RTX 4080 显卡，16 英寸 QHD 显示屏。 \n",
      "48. 联想 IdeaPad 3：英特尔酷睿 i3 处理器，8GB 内存，512GB SSD，集成显卡，\n",
      "15.6 英寸屏幕，适合预算有限用户。 \n",
      "49. 联想 Yoga 9i 14 款：英特尔酷睿 i7 处理器，16GB 内存，512GB SSD，14 英\n",
      "寸 4K 触摸屏，支持 360 度翻转，起售价约 9000 元。 \n",
      "50. 联想 ThinkPad T14：英特尔酷睿 i5 处理器，8GB 内存，512GB SSD，集成显\n",
      "卡，14 英寸 IPS 屏，商务办公本 2。 \n",
      " \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 查看文档内容\n",
    "for doc in documents[-2:]:\n",
    "    print(f\"文档 ID: {doc.doc_id}\")\n",
    "    print(f\"文档内容: {doc.text}\")\n",
    "    print(\"-\" * 50)  # 分隔不同的文档"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754372e8-ca3e-40dc-906e-6106ebad5947",
   "metadata": {},
   "source": [
    "### 生成向量引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "037e3ea2-d6a7-4693-9600-89c299c53c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410936a7-6191-4258-bfac-020ab336cfd9",
   "metadata": {},
   "source": [
    "### 输出检索结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01cb9358-9b6a-4e32-b23d-de430d90f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建检索器\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=3,  # 返回最相似的3个文档\n",
    "    search_kwargs={\"score_threshold\": 0.7},# 相似度阈值,\n",
    "    document_content_description=\"关于联想电脑的产品说明书\", # 文档内容描述\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34d34643-4bd4-41e8-b2d4-fe96d78c9f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  188345.44 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1761 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   511 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  320181.48 ms /  2272 tokens\n"
     ]
    }
   ],
   "source": [
    "# 直接使用查询引擎获取答案\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"如果我想购买14英寸的电脑有哪些？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03b0405-6608-45df-9d51-37eba4e52d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14英寸的电脑有以下几种选择：\n",
      "\n",
      "1. 联想 ideapad flex5：配备AMD锐龙R5或R7处理器，8GB或16GB内存，512GB SSD，集成AMD Radeon Graphics显卡，14英寸IPS触控屏。\n",
      "2. 联想小新 Air15：配备英特尔酷睿i5或i7处理器，8GB或16GB内存，512GB SSD，集成或独立显卡，15.6英寸屏幕。\n",
      "3. 联想 YOGA 9i 14 款：配备英特尔酷睿i7处理器，16GB内存，512GB SSD，14英寸4K触摸屏，支持360度翻转。\n",
      "4. 联想  ThinkPad X1 Carbon ：配备英特尔酷睿i5或i7处理器，8GB或16GB LPDDR4X内存，512GB或1TB SSD，集成英特尔 UHD Graphics 显卡，14英寸高分辨率屏幕。\n",
      "5. 联想 ThinkPad T14：配备英特尔酷睿i5或i7处理器，8GB或16GB DDR4内存，512GB或1TB SSD，集成或独立显卡，14英寸IPS屏。\n",
      "6. 联想小新 14 2024：配备酷睿i5-13420H，16GB内存，512GB SSD，集成显卡，14英寸屏幕。\n",
      "7. 联想小新 Pro14 2024 AI 超能本：配备酷睿i5-13420H等，16GB内存，512GB或1TB SSD，集成显卡，14英寸2.8K屏幕。\n",
      "8. 联想拯救者 Y7000P 2024：配备酷睿i7-14700HX，16GB内存，1TB SSD，显卡RTX4060或RTX4070，15.6英寸IPS屏，高刷新率。\n",
      "9. 联想拯救者 R7000 2024：配备R7-8745H处理器，16GB内存，512GB SSD，RTX4060\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d879c6-bc53-45e7-97f6-3a00d16af7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
